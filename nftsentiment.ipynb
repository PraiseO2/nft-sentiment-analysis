{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NFT Authenticity and Prediction using Sentiment Analysis and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import tqdm\n",
    "import tqdm.asyncio\n",
    "import nest_asyncio\n",
    "import tweepy\n",
    "import time\n",
    "import calendar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm, metrics, preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from aiohttp import ClientSession, TCPConnector\n",
    "from matplotlib import pyplot\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Project List Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"collections_2.csv\")\n",
    "sample_slugs = df['slug'].values\n",
    "slug_date = {}\n",
    "slug_price = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSea Metrics from Collection Slugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetchData(url,session):\n",
    "    '''\n",
    "    This the main task function which takes a URL and an Async Session\n",
    "    and fetches the data from the URL. Since we knew the format of a valid\n",
    "    response, we check if the response was valid and if it isn't, then we \n",
    "    try the request again in another 5 seconds until we get a valid response\n",
    "    '''\n",
    "    async with session.get(url) as response:\n",
    "        try:\n",
    "            response = await response.read()\n",
    "            assert 'detail' not in json.loads(response.decode('UTF-8'))\n",
    "            return response\n",
    "        except:\n",
    "            await asyncio.sleep(5)\n",
    "            return await fetchData(url,session)\n",
    "\n",
    "async def transactionScrape():\n",
    "    '''\n",
    "    API KEY REQUIRED\n",
    "    ---------------------\n",
    "    Utilizes asyncio and aiohttp to make asynchronous requests utilizing the delegated\n",
    "    task function seen above. In this particular use case, we take the creation dates for\n",
    "    projects obtained from the collection scrape function below and increment daily while \n",
    "    fetching transaction data for that time period. As a result, we get 2 weeks worth of \n",
    "    transactions that begin when the project is made public. We parse these transactions to\n",
    "    get the total volume of ETH traded and the number of transaction made.\n",
    "    '''\n",
    "    tasks = []\n",
    "    responses = []\n",
    "    connector = TCPConnector(limit_per_host=1)\n",
    "    url = \"https://api.opensea.io/api/v1/events?collection_slug={slug}&only_opensea=false&event_type=successful&limit=300&occurred_after={start}&occurred_before={end}\"\n",
    "    headers={\"Accept\": \"application/json\", \"X-API-KEY\": os.getenv('OPENSEA_API_KEY')}\n",
    "    async with ClientSession(connector=connector, headers=headers) as session:\n",
    "        for i in range(len(sample_slugs)):\n",
    "            start_date = datetime.fromisoformat(slug_date[sample_slugs[i]].split('T')[0]) - relativedelta(days=1)\n",
    "            for j in range(14):\n",
    "                start_date = start_date + relativedelta(days=1)\n",
    "                end_date = start_date + relativedelta(days=1) \n",
    "                task = asyncio.ensure_future(fetchData(url.format(slug=sample_slugs[i],start=start_date.timestamp(), end=end_date.timestamp()),session))\n",
    "                tasks.append(task)\n",
    "        for f in tqdm.tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "            responses.append(await f)\n",
    "    for response in responses:\n",
    "        response = json.loads(response.decode('utf8'))\n",
    "        try:\n",
    "            collection_slug = response['asset_events'][0]['collection_slug'] \n",
    "            for event in response['asset_events']:\n",
    "                slug_price[collection_slug]['total_volume'] += float(event['total_price'])/1000000000000000000\n",
    "                slug_price[collection_slug]['num_transactions'] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "async def collectionScrape():\n",
    "    '''\n",
    "    Utilizes asyncio and aiohttp to make asynchronous requests utilizing the delegated\n",
    "    task function seen above. In this particular use case, we are simply getting collection\n",
    "    information from OpenSea with the primary goal of getting the creation_date to be used \n",
    "    in transaction scraping. This API endpoint is public so no API key is required for this.\n",
    "    '''\n",
    "    tasks = []\n",
    "    responses = []\n",
    "    connector = TCPConnector()\n",
    "    url = \"https://api.opensea.io/api/v1/collection/{}\"\n",
    "    async with ClientSession(connector=connector) as session:\n",
    "        for i in range(len(sample_slugs)):\n",
    "            task = asyncio.ensure_future(fetchData(url.format(sample_slugs[i]),session))\n",
    "            tasks.append(task)\n",
    "        for f in tqdm.tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "            responses.append(await f)\n",
    "    for response in responses:\n",
    "        response = json.loads(response.decode('utf8'))\n",
    "        collection = response['collection']\n",
    "        name = collection['name']\n",
    "        floor = collection['stats']['floor_price']\n",
    "        volume = collection['stats']['total_volume']\n",
    "        date_created = collection['created_date']\n",
    "        slug_date[collection['slug']] = date_created\n",
    "        slug_price[collection['slug']] = {'total_volume' : 0, 'num_transactions': 0 }\n",
    "        print(f'{name}: Floor: {floor}ETH --- Total Volume: {volume}ETH --- Created {date_created}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Collection Script - outputs to slug_date dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "future = asyncio.ensure_future(collectionScrape())\n",
    "loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Transaction Collections - outputs to slug_price dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "future = asyncio.ensure_future(transactionScrape())\n",
    "loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction and Writing to Features File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['volume'] = df['slug'].apply(lambda x: slug_price[x]['total_volume'])\n",
    "df['transaction_count'] = df['slug'].apply(lambda x: slug_price[x]['num_transactions'])\n",
    "df['average_sale'] = df['slug'].apply(lambda x: slug_price[x]['total_volume'] / slug_price[x]['num_transactions'])\n",
    "df.to_csv('features.csv', mode='a', index=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Historical Tweet Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Authorization Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.getenv('CONSUMER_KEY')\n",
    "consumer_secret = os.getenv('CONSUMER_SECRET')\n",
    "access_token = os.getenv('ACCESS_TOKEN')\n",
    "access_token_secret = os.getenv('ACCESS_TOKEN_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the Projects List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = pd.read_csv(\"collections.csv\")[\"name\"]\n",
    "pprint(collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet Collection Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(proj):\n",
    "    filename = \"./data/\" + \"_\".join(proj.split(\" \")) + \".csv\"\n",
    "    print(\"Collecting tweets for:\", proj)\n",
    "    print(\"Writing results to\", filename)\n",
    "    resp = api.search_full_archive(\"prod\", proj)\n",
    "    tweets = []\n",
    "    timestamps = []\n",
    "    for status in resp:\n",
    "        tweets.append(status.text)\n",
    "        timestamps.append(status.created_at)\n",
    "    df = pd.DataFrame({\n",
    "            \"tweets\": pd.Series(tweets),\n",
    "            \"timestamps\": pd.Series(timestamps)\n",
    "            })\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Tweet Collection Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for proj in collections:\n",
    "    get_tweets(proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sentiment analysis to encode tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tweets):\n",
    "    '''\n",
    "    Uses a VADER model to return the avg sentiment for a collection of tweets\n",
    "    '''\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    all_scores = [0, 0, 0]\n",
    "    for tweet in tweets:\n",
    "        score = analyzer.polarity_scores(tweet)\n",
    "        all_scores.append(np.array([score['neg'], score['neu'], score['pos']]))\n",
    "    return np.array(all_scores).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volume_metric(timestamps):\n",
    "    '''\n",
    "    Function to compute the standard deviation of a series of timestamps\n",
    "    this is meant to act as a metric for volume\n",
    "    in practice this is an imperfect metric because it is heavily reliant upon twitter's api. \n",
    "    we were not able to get a premium license so our data was limited\n",
    "    '''\n",
    "    distances = []\n",
    "    for t in timestamps:\n",
    "        # get the number of seconds since the epoch\n",
    "        distances.append(calendar.timegm(time.strptime(t, \"%Y-%m-%d %H:%M:%S+00:00\")))\n",
    "    return pd.Series(distances).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sentiment encodings to feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"features.csv\")\n",
    "negative_scores = []\n",
    "neutral_scores = []\n",
    "positive_scores = []\n",
    "volume = []\n",
    "\n",
    "for proj in features[\"name\"]:\n",
    "    filename = \"./data/\" + \"_\".join(proj.split(\" \")) + \".csv\"\n",
    "    tweet_data = pd.read_csv(filename)\n",
    "    print(\"Analyzing tweets for\", proj)\n",
    "    scores = get_sentiment(tweet_data[\"tweets\"])\n",
    "    timestamp_deviation = get_volume_metric(tweet_data[\"timestamps\"])\n",
    "    try:\n",
    "        negative_scores.append(scores[0])\n",
    "        neutral_scores.append(scores[1])\n",
    "        positive_scores.append(scores[2])\n",
    "    except Exception:\n",
    "        negative_scores.append(0)\n",
    "        neutral_scores.append(0)\n",
    "        positive_scores.append(0)\n",
    "    if timestamp_deviation:\n",
    "        volume.append(timestamp_deviation)\n",
    "    else:\n",
    "        volume.append(0)\n",
    "        \n",
    "features[\"negative_score\"] = pd.Series(negative_scores)\n",
    "features[\"neutral_score\"] = pd.Series(neutral_scores)\n",
    "features[\"positive_score\"] = pd.Series(positive_scores)\n",
    "features[\"tweet_volume\"] = pd.Series(volume)\n",
    "\n",
    "features.to_csv(\"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNB(x_train, x_test, y_train, y_test):\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(x_train, y_train).predict(x_test)\n",
    "    return metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSVM(x_train, x_test, y_train, y_test):\n",
    "    model = svm.SVC()\n",
    "    model.fit(x_train,y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    return metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictMLP(x_train, x_test, y_train, y_test):\n",
    "    n_features = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train, epochs=1000, batch_size=8, verbose=0)\n",
    "    pyplot.title('Learning Curve')\n",
    "    pyplot.xlabel('Epoch')\n",
    "    pyplot.ylabel('Binary Cross Entropy')\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()   \n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create different training sets using text features only, price features only, and a combination of price and text features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"features.csv\").fillna(0)\n",
    "\n",
    "x_data_price = dataset[['transaction_count', 'volume', 'average_sale']]\n",
    "x_data_text = dataset[['negative_score','neutral_score','positive_score','tweet_volume']]\n",
    "x_data_combined = dataset[['transaction_count', 'volume', 'average_sale','negative_score','neutral_score','positive_score','tweet_volume']]\n",
    "\n",
    "y_data = dataset['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Testing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_price, x_test_price, y_train_price, y_test_price = train_test_split(x_data_price, y_data ,test_size = 0.2)\n",
    "\n",
    "x_train_text, x_test_text, y_train_text, y_test_text = train_test_split(x_data_text, y_data ,test_size = 0.2)\n",
    "\n",
    "x_train_combined, x_test_combined, y_train_combined, y_test_combined = train_test_split(x_data_combined, y_data ,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize The Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(x_train_price)\n",
    "x_train_scaled_price = scaler.transform(x_train_price)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_test_price)\n",
    "x_test_scaled_price = scaler.transform(x_test_price)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_train_text)\n",
    "x_train_scaled_text = scaler.transform(x_train_text)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_test_text)\n",
    "x_test_scaled_text = scaler.transform(x_test_text)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_train_combined)\n",
    "x_train_scaled_combined = scaler.transform(x_train_combined)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x_test_combined)\n",
    "x_test_scaled_combined = scaler.transform(x_test_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictNB(x_train_scaled_text, x_test_scaled_text, y_train_text, y_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSVM(x_train_scaled_text, x_test_scaled_text, y_train_text, y_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictMLP(x_train_scaled_text, x_test_scaled_text, y_train_text, y_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Data Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictNB(x_train_scaled_price, x_test_scaled_price, y_train_price, y_test_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSVM(x_train_scaled_price, x_test_scaled_price, y_train_price, y_test_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictMLP(x_train_scaled_price, x_test_scaled_price, y_train_price, y_test_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price and Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictNB(x_train_scaled_combined, x_test_scaled_combined, y_train_combined, y_test_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSVM(x_train_scaled_combined, x_test_scaled_combined, y_train_combined, y_test_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictMLP(x_train_scaled_combined, x_test_scaled_combined, y_train_combined, y_test_combined)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
